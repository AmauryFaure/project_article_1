{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project_A1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit ('base': conda)",
      "language": "python",
      "name": "python38264bitbaseconda67962706fddd4eb99e2936a1eb425e21"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.8.2-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmauryFaure/project_article_1/blob/master/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SocQoXOEv9xd"
      },
      "source": [
        "# Projet Article 1 :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnG6hNo0PTCh",
        "outputId": "09421b3c-07ef-49d8-b2cb-e4b52895bfe1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqifF1lXv9xt"
      },
      "source": [
        "#Importing pandas and numpy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8_xSR5-8I06",
        "outputId": "fffc8a76-d533-484f-a9ac-dd38e3270f7f"
      },
      "source": [
        "#Check python version\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPR83Bm48boJ",
        "outputId": "368df16a-31ff-4f4a-8f2e-20c9d0169494"
      },
      "source": [
        "#List all packages.\n",
        "pip list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                       Version        \n",
            "----------------------------- ---------------\n",
            "absl-py                       0.10.0         \n",
            "alabaster                     0.7.12         \n",
            "albumentations                0.1.12         \n",
            "altair                        4.1.0          \n",
            "appdirs                       1.4.4          \n",
            "argon2-cffi                   20.1.0         \n",
            "asgiref                       3.3.1          \n",
            "astor                         0.8.1          \n",
            "astropy                       4.1            \n",
            "astunparse                    1.6.3          \n",
            "async-generator               1.10           \n",
            "atari-py                      0.2.6          \n",
            "atomicwrites                  1.4.0          \n",
            "attrs                         20.3.0         \n",
            "audioread                     2.1.9          \n",
            "autograd                      1.3            \n",
            "Babel                         2.9.0          \n",
            "backcall                      0.2.0          \n",
            "beautifulsoup4                4.6.3          \n",
            "bleach                        3.3.0          \n",
            "blis                          0.4.1          \n",
            "bokeh                         2.1.1          \n",
            "Bottleneck                    1.3.2          \n",
            "branca                        0.4.2          \n",
            "bs4                           0.0.1          \n",
            "CacheControl                  0.12.6         \n",
            "cachetools                    4.2.1          \n",
            "catalogue                     1.0.0          \n",
            "certifi                       2020.12.5      \n",
            "cffi                          1.14.4         \n",
            "chainer                       7.4.0          \n",
            "chardet                       3.0.4          \n",
            "click                         7.1.2          \n",
            "cloudpickle                   1.3.0          \n",
            "cmake                         3.12.0         \n",
            "cmdstanpy                     0.9.5          \n",
            "colorlover                    0.3.0          \n",
            "community                     1.0.0b1        \n",
            "contextlib2                   0.5.5          \n",
            "convertdate                   2.2.0          \n",
            "coverage                      3.7.1          \n",
            "coveralls                     0.5            \n",
            "crcmod                        1.7            \n",
            "cufflinks                     0.17.3         \n",
            "cupy-cuda101                  7.4.0          \n",
            "cvxopt                        1.2.5          \n",
            "cvxpy                         1.0.31         \n",
            "cycler                        0.10.0         \n",
            "cymem                         2.0.5          \n",
            "Cython                        0.29.21        \n",
            "daft                          0.0.4          \n",
            "dask                          2.12.0         \n",
            "dataclasses                   0.8            \n",
            "datascience                   0.10.6         \n",
            "debugpy                       1.0.0          \n",
            "decorator                     4.4.2          \n",
            "defusedxml                    0.6.0          \n",
            "descartes                     1.1.0          \n",
            "dill                          0.3.3          \n",
            "distributed                   1.25.3         \n",
            "Django                        3.1.6          \n",
            "dlib                          19.18.0        \n",
            "dm-tree                       0.1.5          \n",
            "docopt                        0.6.2          \n",
            "docutils                      0.16           \n",
            "dopamine-rl                   1.0.5          \n",
            "earthengine-api               0.1.238        \n",
            "easydict                      1.9            \n",
            "ecos                          2.0.7.post1    \n",
            "editdistance                  0.5.3          \n",
            "en-core-web-sm                2.2.5          \n",
            "entrypoints                   0.3            \n",
            "ephem                         3.7.7.1        \n",
            "et-xmlfile                    1.0.1          \n",
            "fa2                           0.3.5          \n",
            "fancyimpute                   0.4.3          \n",
            "fastai                        1.0.61         \n",
            "fastdtw                       0.3.4          \n",
            "fastprogress                  1.0.0          \n",
            "fastrlock                     0.5            \n",
            "fbprophet                     0.7.1          \n",
            "feather-format                0.4.1          \n",
            "filelock                      3.0.12         \n",
            "firebase-admin                4.4.0          \n",
            "fix-yahoo-finance             0.0.22         \n",
            "Flask                         1.1.2          \n",
            "flatbuffers                   1.12           \n",
            "folium                        0.8.3          \n",
            "future                        0.16.0         \n",
            "gast                          0.3.3          \n",
            "GDAL                          2.2.2          \n",
            "gdown                         3.6.4          \n",
            "gensim                        3.6.0          \n",
            "geographiclib                 1.50           \n",
            "geopy                         1.17.0         \n",
            "gin-config                    0.4.0          \n",
            "glob2                         0.7            \n",
            "google                        2.0.3          \n",
            "google-api-core               1.16.0         \n",
            "google-api-python-client      1.7.12         \n",
            "google-auth                   1.24.0         \n",
            "google-auth-httplib2          0.0.4          \n",
            "google-auth-oauthlib          0.4.2          \n",
            "google-cloud-bigquery         1.21.0         \n",
            "google-cloud-bigquery-storage 1.1.0          \n",
            "google-cloud-core             1.0.3          \n",
            "google-cloud-datastore        1.8.0          \n",
            "google-cloud-firestore        1.7.0          \n",
            "google-cloud-language         1.2.0          \n",
            "google-cloud-storage          1.18.1         \n",
            "google-cloud-translate        1.5.0          \n",
            "google-colab                  1.0.0          \n",
            "google-pasta                  0.2.0          \n",
            "google-resumable-media        0.4.1          \n",
            "googleapis-common-protos      1.52.0         \n",
            "googledrivedownloader         0.4            \n",
            "graphviz                      0.10.1         \n",
            "grpcio                        1.32.0         \n",
            "gspread                       3.0.1          \n",
            "gspread-dataframe             3.0.8          \n",
            "gym                           0.17.3         \n",
            "h5py                          2.10.0         \n",
            "HeapDict                      1.0.1          \n",
            "holidays                      0.10.4         \n",
            "holoviews                     1.13.5         \n",
            "html5lib                      1.0.1          \n",
            "httpimport                    0.5.18         \n",
            "httplib2                      0.17.4         \n",
            "httplib2shim                  0.0.3          \n",
            "humanize                      0.5.1          \n",
            "hyperopt                      0.1.2          \n",
            "ideep4py                      2.0.0.post3    \n",
            "idna                          2.10           \n",
            "image                         1.5.33         \n",
            "imageio                       2.4.1          \n",
            "imagesize                     1.2.0          \n",
            "imbalanced-learn              0.4.3          \n",
            "imblearn                      0.0            \n",
            "imgaug                        0.2.9          \n",
            "importlib-metadata            3.4.0          \n",
            "importlib-resources           5.1.0          \n",
            "imutils                       0.5.4          \n",
            "inflect                       2.1.0          \n",
            "iniconfig                     1.1.1          \n",
            "intel-openmp                  2021.1.2       \n",
            "intervaltree                  2.1.0          \n",
            "ipykernel                     4.10.1         \n",
            "ipython                       5.5.0          \n",
            "ipython-genutils              0.2.0          \n",
            "ipython-sql                   0.3.9          \n",
            "ipywidgets                    7.6.3          \n",
            "itsdangerous                  1.1.0          \n",
            "jax                           0.2.8          \n",
            "jaxlib                        0.1.59+cuda101 \n",
            "jdcal                         1.4.1          \n",
            "jedi                          0.18.0         \n",
            "jieba                         0.42.1         \n",
            "Jinja2                        2.11.3         \n",
            "joblib                        1.0.0          \n",
            "jpeg4py                       0.1.4          \n",
            "jsonschema                    2.6.0          \n",
            "jupyter                       1.0.0          \n",
            "jupyter-client                5.3.5          \n",
            "jupyter-console               5.2.0          \n",
            "jupyter-core                  4.7.1          \n",
            "jupyterlab-pygments           0.1.2          \n",
            "jupyterlab-widgets            1.0.0          \n",
            "kaggle                        1.5.10         \n",
            "kapre                         0.1.3.1        \n",
            "Keras                         2.4.3          \n",
            "Keras-Preprocessing           1.1.2          \n",
            "keras-vis                     0.4.1          \n",
            "kiwisolver                    1.3.1          \n",
            "knnimpute                     0.1.0          \n",
            "korean-lunar-calendar         0.2.1          \n",
            "librosa                       0.8.0          \n",
            "lightgbm                      2.2.3          \n",
            "llvmlite                      0.34.0         \n",
            "lmdb                          0.99           \n",
            "lucid                         0.3.8          \n",
            "LunarCalendar                 0.0.9          \n",
            "lxml                          4.2.6          \n",
            "Markdown                      3.3.3          \n",
            "MarkupSafe                    1.1.1          \n",
            "matplotlib                    3.2.2          \n",
            "matplotlib-venn               0.11.6         \n",
            "missingno                     0.4.2          \n",
            "mistune                       0.8.4          \n",
            "mizani                        0.6.0          \n",
            "mkl                           2019.0         \n",
            "mlxtend                       0.14.0         \n",
            "more-itertools                8.6.0          \n",
            "moviepy                       0.2.3.5        \n",
            "mpmath                        1.1.0          \n",
            "msgpack                       1.0.2          \n",
            "multiprocess                  0.70.11.1      \n",
            "multitasking                  0.0.9          \n",
            "murmurhash                    1.0.5          \n",
            "music21                       5.5.0          \n",
            "natsort                       5.5.0          \n",
            "nbclient                      0.5.1          \n",
            "nbconvert                     5.6.1          \n",
            "nbformat                      5.1.2          \n",
            "nest-asyncio                  1.5.1          \n",
            "networkx                      2.5            \n",
            "nibabel                       3.0.2          \n",
            "nltk                          3.2.5          \n",
            "notebook                      5.3.1          \n",
            "np-utils                      0.5.12.1       \n",
            "numba                         0.51.2         \n",
            "numexpr                       2.7.2          \n",
            "numpy                         1.19.5         \n",
            "nvidia-ml-py3                 7.352.0        \n",
            "oauth2client                  4.1.3          \n",
            "oauthlib                      3.1.0          \n",
            "okgrade                       0.4.3          \n",
            "opencv-contrib-python         4.1.2.30       \n",
            "opencv-python                 4.1.2.30       \n",
            "openpyxl                      2.5.9          \n",
            "opt-einsum                    3.3.0          \n",
            "osqp                          0.6.2.post0    \n",
            "packaging                     20.9           \n",
            "palettable                    3.3.0          \n",
            "pandas                        1.1.5          \n",
            "pandas-datareader             0.9.0          \n",
            "pandas-gbq                    0.13.3         \n",
            "pandas-profiling              1.4.1          \n",
            "pandocfilters                 1.4.3          \n",
            "panel                         0.9.7          \n",
            "param                         1.10.1         \n",
            "parso                         0.8.1          \n",
            "pathlib                       1.0.1          \n",
            "patsy                         0.5.1          \n",
            "pexpect                       4.8.0          \n",
            "pickleshare                   0.7.5          \n",
            "Pillow                        7.0.0          \n",
            "pip                           19.3.1         \n",
            "pip-tools                     4.5.1          \n",
            "plac                          1.1.3          \n",
            "plotly                        4.4.1          \n",
            "plotnine                      0.6.0          \n",
            "pluggy                        0.7.1          \n",
            "pooch                         1.3.0          \n",
            "portpicker                    1.3.1          \n",
            "prefetch-generator            1.0.1          \n",
            "preshed                       3.0.5          \n",
            "prettytable                   2.0.0          \n",
            "progressbar2                  3.38.0         \n",
            "prometheus-client             0.9.0          \n",
            "promise                       2.3            \n",
            "prompt-toolkit                1.0.18         \n",
            "protobuf                      3.12.4         \n",
            "psutil                        5.4.8          \n",
            "psycopg2                      2.7.6.1        \n",
            "ptyprocess                    0.7.0          \n",
            "py                            1.10.0         \n",
            "pyarrow                       0.14.1         \n",
            "pyasn1                        0.4.8          \n",
            "pyasn1-modules                0.2.8          \n",
            "pycocotools                   2.0.2          \n",
            "pycparser                     2.20           \n",
            "pyct                          0.4.8          \n",
            "pydata-google-auth            1.1.0          \n",
            "pydot                         1.3.0          \n",
            "pydot-ng                      2.0.0          \n",
            "pydotplus                     2.0.2          \n",
            "PyDrive                       1.3.1          \n",
            "pyemd                         0.5.1          \n",
            "pyglet                        1.5.0          \n",
            "Pygments                      2.6.1          \n",
            "pygobject                     3.26.1         \n",
            "pymc3                         3.7            \n",
            "PyMeeus                       0.3.7          \n",
            "pymongo                       3.11.2         \n",
            "pymystem3                     0.2.0          \n",
            "pynndescent                   0.5.1          \n",
            "PyOpenGL                      3.1.5          \n",
            "pyparsing                     2.4.7          \n",
            "pyrsistent                    0.17.3         \n",
            "pysndfile                     1.3.8          \n",
            "PySocks                       1.7.1          \n",
            "pystan                        2.19.1.1       \n",
            "pytest                        3.6.4          \n",
            "python-apt                    1.6.5+ubuntu0.5\n",
            "python-chess                  0.23.11        \n",
            "python-dateutil               2.8.1          \n",
            "python-louvain                0.15           \n",
            "python-slugify                4.0.1          \n",
            "python-utils                  2.5.5          \n",
            "pytz                          2018.9         \n",
            "pyviz-comms                   2.0.1          \n",
            "PyWavelets                    1.1.1          \n",
            "PyYAML                        3.13           \n",
            "pyzmq                         22.0.2         \n",
            "qdldl                         0.1.5.post0    \n",
            "qtconsole                     5.0.2          \n",
            "QtPy                          1.9.0          \n",
            "regex                         2019.12.20     \n",
            "requests                      2.23.0         \n",
            "requests-oauthlib             1.3.0          \n",
            "resampy                       0.2.2          \n",
            "retrying                      1.3.3          \n",
            "rpy2                          3.2.7          \n",
            "rsa                           4.7            \n",
            "scikit-image                  0.16.2         \n",
            "scikit-learn                  0.22.2.post1   \n",
            "scipy                         1.4.1          \n",
            "screen-resolution-extra       0.0.0          \n",
            "scs                           2.1.2          \n",
            "seaborn                       0.11.1         \n",
            "Send2Trash                    1.5.0          \n",
            "setuptools                    53.0.0         \n",
            "setuptools-git                1.2            \n",
            "Shapely                       1.7.1          \n",
            "simplegeneric                 0.8.1          \n",
            "six                           1.15.0         \n",
            "sklearn                       0.0            \n",
            "sklearn-pandas                1.8.0          \n",
            "smart-open                    4.1.2          \n",
            "snowballstemmer               2.1.0          \n",
            "sortedcontainers              2.3.0          \n",
            "SoundFile                     0.10.3.post1   \n",
            "spacy                         2.2.4          \n",
            "Sphinx                        1.8.5          \n",
            "sphinxcontrib-serializinghtml 1.1.4          \n",
            "sphinxcontrib-websupport      1.2.4          \n",
            "SQLAlchemy                    1.3.23         \n",
            "sqlparse                      0.4.1          \n",
            "srsly                         1.0.5          \n",
            "statsmodels                   0.10.2         \n",
            "sympy                         1.1.1          \n",
            "tables                        3.4.4          \n",
            "tabulate                      0.8.7          \n",
            "tblib                         1.7.0          \n",
            "tensorboard                   2.4.1          \n",
            "tensorboard-plugin-wit        1.8.0          \n",
            "tensorboardcolab              0.0.22         \n",
            "tensorflow                    2.4.1          \n",
            "tensorflow-addons             0.8.3          \n",
            "tensorflow-datasets           4.0.1          \n",
            "tensorflow-estimator          2.4.0          \n",
            "tensorflow-gcs-config         2.4.0          \n",
            "tensorflow-hub                0.11.0         \n",
            "tensorflow-metadata           0.27.0         \n",
            "tensorflow-privacy            0.2.2          \n",
            "tensorflow-probability        0.12.1         \n",
            "termcolor                     1.1.0          \n",
            "terminado                     0.9.2          \n",
            "testpath                      0.4.4          \n",
            "text-unidecode                1.3            \n",
            "textblob                      0.15.3         \n",
            "textgenrnn                    1.4.1          \n",
            "Theano                        1.0.5          \n",
            "thinc                         7.4.0          \n",
            "tifffile                      2020.9.3       \n",
            "toml                          0.10.2         \n",
            "toolz                         0.11.1         \n",
            "torch                         1.7.0+cu101    \n",
            "torchsummary                  1.5.1          \n",
            "torchtext                     0.3.1          \n",
            "torchvision                   0.8.1+cu101    \n",
            "tornado                       5.1.1          \n",
            "tqdm                          4.41.1         \n",
            "traitlets                     4.3.3          \n",
            "tweepy                        3.6.0          \n",
            "typeguard                     2.7.1          \n",
            "typing-extensions             3.7.4.3        \n",
            "tzlocal                       1.5.1          \n",
            "umap-learn                    0.5.0          \n",
            "uritemplate                   3.0.1          \n",
            "urllib3                       1.24.3         \n",
            "vega-datasets                 0.9.0          \n",
            "wasabi                        0.8.2          \n",
            "wcwidth                       0.2.5          \n",
            "webencodings                  0.5.1          \n",
            "Werkzeug                      1.0.1          \n",
            "wheel                         0.36.2         \n",
            "widgetsnbextension            3.5.1          \n",
            "wordcloud                     1.5.0          \n",
            "wrapt                         1.12.1         \n",
            "xarray                        0.15.1         \n",
            "xgboost                       0.90           \n",
            "xkit                          0.0.0          \n",
            "xlrd                          1.1.0          \n",
            "xlwt                          1.3.0          \n",
            "yellowbrick                   0.9.1          \n",
            "zict                          2.0.0          \n",
            "zipp                          3.4.0          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps97S30yv9xu"
      },
      "source": [
        "## Creating the training Dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_hTvFZmTv9xu"
      },
      "source": [
        "#Get tweets\n",
        "df_to_moderate=pd.read_csv(\"/content/drive/MyDrive/article_1_data/selected_tweets.csv\")\n",
        "#Drop unused data\n",
        "df_to_moderate=df_to_moderate.drop([\"Id\",\"source\",\"Looked up word\"],axis=1)\n",
        "#Change column name\n",
        "df_to_moderate[\"content\"]=df_to_moderate[\"tweet (without @)\"]\n",
        "df_to_moderate=df_to_moderate.drop([\"tweet (without @)\"], axis=1)\n",
        "#Drop empty lines\n",
        "df_to_moderate=df_to_moderate.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oU7Wsxv9xw"
      },
      "source": [
        "#Get regular data\n",
        "df_article1_messages=pd.read_excel(\"/content/drive/MyDrive/article_1_data/AmauryModerationAllMessagesInspireFrom3Aout2020.xlsx\", sheet_name=\"ContenuNormal\")\n",
        "#Drop unused data\n",
        "df_article1_messages=df_article1_messages.drop([\"_id\",\"sender\",\"recipients.0\",\"threadId\",\"timestamp\",\"EchantillonNormal\"], axis=1)\n",
        "#Assign label\n",
        "df_article1_messages[\"Harmful\"]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZbR7w16v9xy"
      },
      "source": [
        "#Create training dataset. Balanced 50% for each class !\n",
        "df_to_moderate=pd.concat([df_article1_messages[:200],df_to_moderate],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53OrpKtv9xz"
      },
      "source": [
        "## Creating the test Dataset\n",
        "\n",
        "Problem : we do not have a balanced Dataset of real examples from article 1 for which we know the real distribution.\n",
        "\n",
        "What will we done instead :\n",
        "\n",
        "- Test on the dataset of MLMA tweets. \n",
        "- See % that come back on Article 1 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ZCUPLVv9xz"
      },
      "source": [
        "test=pd.read_csv(\"/content/drive/MyDrive/article_1_data/fr_dataset_test.csv\")\n",
        "#Dropping Unused columns\n",
        "test=test.drop(test.columns[[7,8,9]],axis=1)\n",
        "test=test.drop(columns=[\"HITId\",\"directness\",\"annotator_sentiment\",\"target\",\"group\"])\n",
        "#Changing sentiment to 0 (for normal) and 1 (for else) \n",
        "test[\"sentiment\"]=[0 if test[\"sentiment\"][x]==\"normal\" else 1 for x in range(test.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQaZ3Dhwv9x0"
      },
      "source": [
        "## Implementing BoW and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdCToMlLv9x0"
      },
      "source": [
        "#Creating arrays for test_values and labels\n",
        "test_values=np.array(test[\"tweet\"])\n",
        "test_labels=np.array(test[\"sentiment\"])\n",
        "# Same but for training\n",
        "train_values=np.array(df_to_moderate[\"content\"])\n",
        "#Extra step to Making sure everything is on a string format (some numbers might not be)\n",
        "train_values=[str(train_values[i]) for i in range(len(train_values))]\n",
        "train_labels=np.array(df_to_moderate[\"Harmful\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8lMYH_v9x0"
      },
      "source": [
        "\n",
        "from joblib import dump, load\n",
        "\n",
        "#Import SGDClassifier (allows to implement logistic regression)\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#Import matplotlib for plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#Import some metrics\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
        "#Import seaborn for plots\n",
        "import seaborn as sns\n",
        "\n",
        "#This function comes from : https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2\n",
        "#It computes F1-score\n",
        "def calc_f1(p_and_r):\n",
        "    p, r = p_and_r\n",
        "    return (2*p*r)/(p+r)\n",
        "\n",
        "#This function comes from : https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2\n",
        "#It computes some metrics\n",
        "def compute_metrics(y_test, y_prob, verbose=False, return_metrics=True, confusion=False):\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_prob, pos_label = 1)\n",
        "\n",
        "  #Optimizing the F1-score\n",
        "  best_f1_index =np.argmax([calc_f1(p_r) for p_r in zip(precision, recall)])\n",
        "  best_threshold, best_precision, best_recall = threshold[best_f1_index], precision[best_f1_index], recall[best_f1_index]\n",
        "\n",
        "  y_test_pred = np.where(y_prob > best_threshold, 1, 0)\n",
        "  \n",
        "  f1 = f1_score(y_test, y_test_pred, pos_label = 1, average = 'binary')\n",
        "  roc_auc = roc_auc_score(y_test, y_prob)\n",
        "  acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "  if confusion:\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(cm)\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    sns.set(font_scale=1.0) #for label size\n",
        "    sns.heatmap(cm, annot = True, fmt = 'd', xticklabels = ['Not harmful', 'Harmful'], yticklabels = ['Not harmful', 'Harmful'], annot_kws={\"size\": 14}, cmap = 'Blues')# font size\n",
        "\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Truth')\n",
        "\n",
        "  if verbose:\n",
        "    print('F1: {:.3f} | Pr: {:.3f} | Re: {:.3f} | AUC: {:.3f} | Accuracy: {:.3f} \\n'.format(f1, best_precision, best_recall, roc_auc, acc))\n",
        "    \n",
        "  if return_metrics:\n",
        "    return np.array([f1, best_precision, best_recall, roc_auc, acc])\n",
        "\n",
        "#Defining a function to do 10 times the logistic regression\n",
        "def evaluate_log_reg(train_features, test_features, y_train, y_test):\n",
        "    score=0\n",
        "    metrics = np.zeros(5)\n",
        "    for i in range(10):\n",
        "        log_reg=SGDClassifier(loss=\"log\", penalty='l2')\n",
        "        log_reg.fit(train_features,y_train)\n",
        "      \n",
        "        y_prob=log_reg.predict_proba(test_features)[:,1]\n",
        "        metrics+=compute_metrics(y_test,y_prob)\n",
        "    metrics /=10\n",
        "\n",
        "    return metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_J22Bvv9x0",
        "outputId": "ff7bcc18-a6f8-48dd-cd74-6ae295258cf4"
      },
      "source": [
        "# Bag of Word\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Declaring the vectorizer\n",
        "bow=CountVectorizer()\n",
        "#Fiting the model to the training dataset, and vectorizing the training dataset\n",
        "train_bow=bow.fit_transform(train_values)\n",
        "#Vectorizing test dataset\n",
        "test_bow=bow.transform(test_values)\n",
        "#Real metrics\n",
        "metrics_bow=evaluate_log_reg(train_bow, test_bow, train_labels, test_labels)\n",
        "#Metrics evaluated on train dataset (to see if overfitting)\n",
        "metrics_train=evaluate_log_reg(train_bow, train_bow, train_labels, train_labels)\n",
        "print(metrics_bow)\n",
        "print(metrics_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.88641413 0.79712106 0.99863622 0.59643891 0.79649344]\n",
            "[0.99749373 1.         1.         1.         0.9975    ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxjIM_xcv9x1",
        "outputId": "4799a216-4cd6-4e19-c684-b887cb961bf7"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf=TfidfVectorizer()\n",
        "train_tfidf=tfidf.fit_transform(train_values)\n",
        "test_tfidf=tfidf.transform(test_values)\n",
        "\n",
        "metrics_tfidf=evaluate_log_reg(train_tfidf, test_tfidf, train_labels, test_labels)\n",
        "metrics_train_tfidf=evaluate_log_reg(train_tfidf, train_tfidf, train_labels, train_labels)\n",
        "print(metrics_tfidf)\n",
        "print(metrics_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.88657879 0.79748746 0.99841421 0.65177877 0.79687185]\n",
            "[0.99749373 1.         1.         1.         0.9975    ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvX7M3I4v9x4"
      },
      "source": [
        "### Implementing camembert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBfwKk0Vv9x4"
      },
      "source": [
        "# Remember : \n",
        "# test_values=np.array(test[\"tweet\"])\n",
        "# test_labels=np.array(test[\"sentiment\"])\n",
        "\n",
        "# train_values=np.array(df_to_moderate[\"content\"])\n",
        "# train_values=[str(train_values[i]) for i in range(len(train_values))]\n",
        "# train_labels=np.array(df_to_moderate[\"Harmful\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZJ4qQFv9x4"
      },
      "source": [
        "#Importing function to split training data into validation and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_values, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMt56ub2ycLp",
        "outputId": "d4430ea7-2c53-46ec-c9c8-60b16ab243e0"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxo04C5ov9x4"
      },
      "source": [
        "#Tokeinization for camembert\n",
        "from transformers import CamembertTokenizer\n",
        "tokenizer=CamembertTokenizer.from_pretrained(\"camembert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iRHGAFv9x4"
      },
      "source": [
        "#Tokenizing\n",
        "train_encodings = tokenizer(train_texts, padding=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True)\n",
        "test_encodings = tokenizer(test_values.tolist(), padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1DSz2W_v9x5"
      },
      "source": [
        "#Importing pyTorch\n",
        "import torch\n",
        "\n",
        "class A1Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "#Crating dataset\n",
        "train_dataset =  A1Dataset(train_encodings, train_labels)\n",
        "val_dataset =  A1Dataset(val_encodings, val_labels)\n",
        "test_dataset =  A1Dataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R00kW3qv9x5"
      },
      "source": [
        "# from transformers import CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',          # output directory\n",
        "#     num_train_epochs=3,              # total number of training epochs\n",
        "#     per_device_train_batch_size=16,  # batch size per device during training\n",
        "#     per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "#     weight_decay=0.01,               # strength of weight decay\n",
        "#     logging_dir='./logs',            # directory for storing logs\n",
        "#     logging_steps=10,\n",
        "# )\n",
        "\n",
        "# model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\")\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "#     args=training_args,                  # training arguments, defined above\n",
        "#     train_dataset=train_dataset,         # training dataset\n",
        "#     eval_dataset=val_dataset             # evaluation dataset\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "odufWocEv9x6",
        "outputId": "c948968e-122d-4085-f9a9-85a896aa8ac8"
      },
      "source": [
        "#Utility to load dataset in batch\n",
        "from torch.utils.data import DataLoader\n",
        "#Importing camembdert and AdamW Optimizer\n",
        "from transformers import CamembertForSequenceClassification, AdamW\n",
        "\n",
        "#Model declaration\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "#Training\n",
        "for epoch in range(1):\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        print(labels)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.long())\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d0200f59fbaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Importing camembdert and AdamW Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCamembertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Model declaration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NyRUliRv9x6"
      },
      "source": [
        "# test_data=DataLoader(test_dataset, batch_size=1)\n",
        "# acc=0\n",
        "# for data in test_data:\n",
        "#   input_ids = data['input_ids'].to(device)\n",
        "#   attention_mask = data['attention_mask'].to(device)\n",
        "#   labels = data['labels'].to(device)\n",
        "#   outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#   # print(labels)\n",
        "#   if torch.argmax(outputs[0])==labels:\n",
        "#     acc+=1\n",
        "#   # print(torch.argmax(outputs[0]))\n",
        "#   compute_metrics(labels,torch.argmax(outputs[0]).numpy())\n",
        "\n",
        "# acc=acc/(len(test_data))\n",
        "# print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Ti767eguQZry",
        "outputId": "c05abb3f-a141-469b-8620-ad1bf3d2c9f3"
      },
      "source": [
        "#Evaluating the model : \n",
        "with torch.no_grad():\n",
        "  input_ids = test_dataset[:50]['input_ids'].to(device)\n",
        "  attention_mask = test_dataset[:50]['attention_mask'].to(device)\n",
        "  labels = test_dataset[:50]['labels'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  # print(np.argmax(outputs[0].cpu().data.numpy(), axis=1))\n",
        "\n",
        "  # print(labels.cpu().data.numpy())\n",
        "  # print(torch.argmax(outputs[0]).cpu().data.numpy())\n",
        "  metrics_camembert=compute_metrics(labels.cpu().data.numpy(),outputs[0].cpu().data.numpy()[:,1], confusion=True)\n",
        "  print(metrics_camembert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2 12]\n",
            " [ 2 34]]\n",
            "[0.82926829 0.74468085 0.97222222 0.63293651 0.72      ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96i1UVXx2ExG"
      },
      "source": [
        "# # Are we overfitting ? \n",
        "\n",
        "# train_data = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# for data in train_data:\n",
        "#   input_ids = data['input_ids'].to(device)\n",
        "#   attention_mask = data['attention_mask'].to(device)\n",
        "#   labels = data['labels'].to(device)\n",
        "#   outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#   # print(labels)\n",
        "#   if torch.argmax(outputs[0])==labels:\n",
        "#     acc+=1\n",
        "#   # print(torch.argmax(outputs[0]))\n",
        "\n",
        "# acc=acc/(len(train_data))\n",
        "# print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB54-0QQcEGD"
      },
      "source": [
        "We are clearly overfitting here.\n",
        "The problem is it is hard to know if it is a good or a bad thing ? Indeed the dataset is not really what we want to moderate, so maybe it is okay... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Sa00rwsDv7"
      },
      "source": [
        "metrics_df=pd.DataFrame(metrics_bow.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "df2=pd.DataFrame(metrics_tfidf.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "df3=pd.DataFrame(metrics_camembert.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "metrics_df=metrics_df.append(df2,ignore_index=True)\n",
        "metrics_df=metrics_df.append(df3,ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "JxwhcYazbtI8",
        "outputId": "97801f1f-864c-4724-8fd1-7346f8cfeb2f"
      },
      "source": [
        "metrics_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>best_precision</th>\n",
              "      <th>best_recall</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.886337</td>\n",
              "      <td>0.796855</td>\n",
              "      <td>0.998858</td>\n",
              "      <td>0.598020</td>\n",
              "      <td>0.796292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.886563</td>\n",
              "      <td>0.797442</td>\n",
              "      <td>0.998509</td>\n",
              "      <td>0.651417</td>\n",
              "      <td>0.796821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.884426</td>\n",
              "      <td>0.795409</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.548913</td>\n",
              "      <td>0.798184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1  best_precision  best_recall   roc_auc       acc\n",
              "0  0.886337        0.796855     0.998858  0.598020  0.796292\n",
              "1  0.886563        0.797442     0.998509  0.651417  0.796821\n",
              "2  0.884426        0.795409     1.000000  0.548913  0.798184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyQJg1i-OkzN"
      },
      "source": [
        "On n'a pas de meilleure performance selon le modèle choisi. \n",
        "Etonnament le modèle camembert donne une aire sous la courbe ROC plus faible que les autres, alors que le reste des indicateurs sont similaires.\n",
        "\n",
        "Piste d'amélioration :\n",
        "- Feature Engineering\n",
        "- Utiliser camembert + Log reg ? \n",
        "-Try 2 epochs ? It is worse... F1 AT 0.1, confusion matrix shows worse. Same than 1 on second run\n",
        "- Try 3 epoch ? Not great... \n",
        "1 epoch seems to do the job just fine. \n",
        "-Try padding = done\n",
        "- Truncation does not change anything.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tDCa0DmxiSt"
      },
      "source": [
        "## Analyse : Quelles implications sur les messages à modérer A1 ? \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "JqNwKhXeyutY",
        "outputId": "fe273900-99dd-4d12-e0eb-4cf87cc8e632"
      },
      "source": [
        "df_a1_to_moderate=pd.read_excel(\"/content/drive/MyDrive/article_1_data/AmauryModerationAllMessagesInspireFrom3Aout2020.xlsx\", sheet_name=\"ContenuToModerate\")\r\n",
        "df_a1_to_moderate=df_a1_to_moderate.drop(columns=[\"_id\",\"sender\",\"recipients.0\",\"threadId\",\"timestamp\",\"EchantillonToModerate\"])\r\n",
        "df_a1_to_moderate.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non, la physique-chimie de lycée n’a pas grand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Merci infiniment d'avoir pris autant de temps ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bonjour ! Alors non la SVT et la biologie ce n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>salut\\nJ'aimerais savoir ce qu'est exactement ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>je sais pas si j'ai répondu à toutes tes quest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content\n",
              "0  Non, la physique-chimie de lycée n’a pas grand...\n",
              "1  Merci infiniment d'avoir pris autant de temps ...\n",
              "2  Bonjour ! Alors non la SVT et la biologie ce n...\n",
              "3  salut\\nJ'aimerais savoir ce qu'est exactement ...\n",
              "4  je sais pas si j'ai répondu à toutes tes quest..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO_9WPqD0Fjm"
      },
      "source": [
        "test_values=np.array(test[\"tweet\"])\r\n",
        "test_labels=np.array(test[\"sentiment\"])\r\n",
        "\r\n",
        "train_values=np.array(df_to_moderate[\"content\"])\r\n",
        "train_values=[str(train_values[i]) for i in range(len(train_values))]\r\n",
        "train_labels=np.array(df_to_moderate[\"Harmful\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfgEdBDFxpO4"
      },
      "source": [
        "\r\n",
        "bow=CountVectorizer()\r\n",
        "train_bow=bow.fit_transform(train_values)\r\n",
        "test_bow=bow.transform(test_values)\r\n",
        "\r\n",
        "# metrics_bow=evaluate_log_reg(train_bow, test_bow, train_labels, test_labels)\r\n",
        "\r\n",
        "log_reg=SGDClassifier(loss=\"log\", penalty='l2')\r\n",
        "log_reg.fit(train_bow,train_labels)\r\n",
        "\r\n",
        "prediction=log_reg.predict(bow.transform(df_a1_to_moderate[\"content\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P1DrhvJ0XQk",
        "outputId": "5c057015-4c1b-4e69-edad-a3e7a78c5ec7"
      },
      "source": [
        "print(prediction)\r\n",
        "print(f\"Le modèle fait ressortir 737 messages problématiques : {sum(prediction)}\")\r\n",
        "print(f\"Nombre de messages problématiques originaux : {df_a1_to_moderate.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "Le modèle fait ressortir 737 messages problématiques : 703.0\n",
            "Nombre de messages problématiques originaux : 8233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Rs8zgz1L1N",
        "outputId": "8ba1cb1d-da60-44d2-b2e6-90cded5154e4"
      },
      "source": [
        "indexes=[ i for i in range(len(prediction)) if prediction[i]==1]\r\n",
        "df_a1_bow_moderate=df_a1_to_moderate.iloc[indexes]\r\n",
        "print(indexes)\r\n",
        "\r\n",
        "for i in range(10): \r\n",
        "  print(df_a1_bow_moderate.iloc[i][\"content\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 34, 36, 38, 45, 77, 79, 82, 123, 126, 141, 143, 145, 164, 166, 194, 204, 210, 220, 233, 243, 252, 275, 284, 299, 303, 316, 327, 331, 333, 337, 339, 340, 343, 353, 365, 375, 387, 388, 431, 437, 450, 453, 469, 491, 499, 504, 505, 521, 526, 527, 562, 566, 572, 574, 591, 595, 608, 641, 642, 646, 693, 694, 696, 697, 698, 699, 701, 729, 735, 749, 764, 770, 779, 803, 808, 809, 829, 838, 904, 956, 957, 976, 983, 987, 1030, 1040, 1041, 1043, 1046, 1056, 1081, 1104, 1109, 1116, 1125, 1129, 1144, 1186, 1189, 1247, 1270, 1273, 1291, 1300, 1305, 1308, 1318, 1319, 1329, 1377, 1388, 1410, 1421, 1426, 1430, 1447, 1458, 1460, 1472, 1482, 1494, 1495, 1520, 1538, 1542, 1561, 1564, 1569, 1579, 1594, 1602, 1605, 1607, 1608, 1612, 1618, 1620, 1686, 1688, 1689, 1696, 1705, 1706, 1725, 1737, 1740, 1741, 1742, 1749, 1760, 1765, 1767, 1783, 1850, 1854, 1856, 1891, 1896, 1897, 1915, 1916, 1917, 1923, 1931, 1940, 1954, 1955, 1958, 1965, 1990, 1993, 2008, 2029, 2040, 2042, 2057, 2064, 2090, 2093, 2095, 2103, 2106, 2107, 2111, 2116, 2138, 2139, 2140, 2143, 2169, 2198, 2199, 2214, 2219, 2225, 2228, 2229, 2248, 2268, 2279, 2284, 2286, 2290, 2293, 2294, 2298, 2302, 2305, 2306, 2307, 2314, 2344, 2347, 2379, 2383, 2411, 2412, 2427, 2432, 2446, 2448, 2465, 2478, 2491, 2497, 2498, 2500, 2504, 2513, 2522, 2531, 2536, 2538, 2539, 2544, 2545, 2554, 2585, 2612, 2627, 2653, 2655, 2680, 2691, 2693, 2706, 2707, 2720, 2721, 2723, 2750, 2763, 2791, 2794, 2812, 2818, 2823, 2832, 2836, 2837, 2872, 2877, 2880, 2881, 2886, 2891, 2892, 2893, 2911, 2912, 2925, 2965, 2975, 3012, 3078, 3086, 3089, 3097, 3099, 3100, 3133, 3136, 3137, 3139, 3141, 3151, 3167, 3194, 3199, 3211, 3227, 3246, 3250, 3254, 3271, 3299, 3300, 3302, 3310, 3311, 3338, 3372, 3375, 3386, 3395, 3426, 3428, 3444, 3457, 3474, 3482, 3497, 3499, 3533, 3536, 3556, 3560, 3563, 3587, 3607, 3646, 3648, 3684, 3693, 3699, 3710, 3718, 3729, 3736, 3750, 3771, 3773, 3776, 3780, 3790, 3795, 3816, 3845, 3860, 3867, 3870, 3874, 3883, 3901, 3914, 3916, 3918, 3929, 3930, 3935, 3953, 3984, 3987, 3988, 3993, 4006, 4007, 4008, 4011, 4013, 4015, 4024, 4040, 4047, 4057, 4058, 4091, 4104, 4107, 4119, 4129, 4136, 4148, 4151, 4155, 4184, 4190, 4197, 4198, 4200, 4201, 4206, 4209, 4212, 4213, 4217, 4218, 4219, 4220, 4221, 4226, 4227, 4240, 4258, 4278, 4280, 4287, 4294, 4321, 4327, 4337, 4378, 4401, 4423, 4425, 4465, 4481, 4504, 4509, 4531, 4532, 4534, 4551, 4561, 4566, 4570, 4574, 4584, 4588, 4619, 4649, 4653, 4675, 4683, 4686, 4691, 4734, 4774, 4776, 4782, 4783, 4785, 4804, 4806, 4825, 4881, 4882, 4885, 4887, 4888, 4891, 4926, 4936, 4943, 4957, 4964, 4966, 4968, 4974, 4991, 5018, 5019, 5026, 5030, 5032, 5060, 5078, 5083, 5094, 5116, 5118, 5119, 5123, 5157, 5176, 5180, 5187, 5202, 5207, 5210, 5228, 5249, 5252, 5267, 5283, 5286, 5328, 5346, 5347, 5351, 5352, 5353, 5356, 5357, 5358, 5374, 5377, 5397, 5398, 5431, 5436, 5441, 5443, 5447, 5459, 5462, 5464, 5470, 5483, 5493, 5524, 5530, 5541, 5601, 5603, 5606, 5627, 5637, 5639, 5640, 5643, 5661, 5674, 5688, 5696, 5698, 5699, 5700, 5724, 5763, 5769, 5787, 5792, 5793, 5796, 5801, 5836, 5850, 5854, 5859, 5861, 5870, 5887, 5890, 5903, 5909, 5969, 5985, 6055, 6088, 6095, 6104, 6112, 6127, 6128, 6132, 6133, 6149, 6152, 6153, 6154, 6158, 6177, 6201, 6202, 6206, 6210, 6221, 6249, 6251, 6320, 6334, 6390, 6423, 6429, 6451, 6484, 6488, 6489, 6499, 6503, 6507, 6519, 6528, 6534, 6566, 6572, 6601, 6603, 6611, 6641, 6653, 6659, 6672, 6690, 6693, 6700, 6702, 6707, 6709, 6723, 6745, 6754, 6776, 6787, 6802, 6808, 6810, 6812, 6845, 6883, 6889, 6922, 6926, 6930, 6931, 6936, 6938, 6957, 7003, 7029, 7049, 7052, 7075, 7092, 7096, 7097, 7099, 7100, 7104, 7105, 7113, 7145, 7168, 7189, 7198, 7201, 7205, 7207, 7212, 7222, 7232, 7234, 7272, 7279, 7295, 7298, 7347, 7365, 7369, 7373, 7374, 7377, 7382, 7399, 7406, 7421, 7480, 7482, 7501, 7506, 7509, 7551, 7553, 7555, 7556, 7569, 7580, 7601, 7602, 7603, 7624, 7642, 7643, 7670, 7715, 7718, 7779, 7787, 7804, 7805, 7806, 7807, 7831, 7834, 7838, 7841, 7845, 7847, 7849, 7853, 7854, 7893, 7907, 7909, 7947, 7959, 7982, 7983, 7986, 8027, 8033, 8034, 8042, 8108, 8115, 8143, 8147, 8170, 8205, 8213, 8222]\n",
            "Non j'étais logée chez mes parents \n",
            "Mais sinon je vais qu'il y a pas mal de résidences étudiantes autour de l'IUT\n",
            "Les oraux c'est en mai il me semble ! En fait tu choisis une image d'un bâtiment (ou autre) qu'ils te proposent et tu dois en parler. Mais c'est pas forcément un lien avec l'archi dans le sens que tu dois donner ton ressenti vis a vis des couleurs, du lieu auquel ca te fait penser... j'ai discuté avec un prof jury qui m'a dit que le plus important c'est de parler de soi donc donne les souvenirs que ça te ramène, parle de tes voyages, montre que tu t'intéresse au monde qui t'entoure\n",
            "t'es un amour merciiiiiiiiii\n",
            "Après ce qui est difficile au début c'est que on ne t'apprend pas les choses, on te laisse plutot découvrir par toi même... ma première maquette c'est un défi mdrr \n",
            "Après niveau ambiance c'est super ! Énormément d'entre aide entre les différents niveaux et au sein même de la promo !\n",
            "Le double diplôme archi ingé ça m'intéresse mais ça a l'air d'être hyper sélectif...\n",
            "Mais ducoup ça porte sur quoi les oraux ?\n",
            "*inébranlable est un caprice de mon correcteur automatique\n",
            "Pour les débouchés, c’est la question qui fâche haha. Après une licence d’histoire, on continue généralement vers un master spécialisé dans la période, le sujet qui t’intéresse. Il est également possible  de  passer des concours pour grandes écoles ou écoles de journalisme par exemple. Le débouché que l´on mentionne  le plus souvent est celui de prof, mais ce n'est pas l´unique.\n",
            "Bonjour, par le même domaine tu veux dire la double licence histoire-llcer espagnol ?\n",
            "Donc se lancer dans le grand bain le plus tôt possible me semble être une bonne chose.\n",
            "En ce qui concerne l'orientation je ne suis fermé à rien. Le \"Graal\" à atteindre ce serait Sciences Po (celui de Lyon de préférence),extrêmement difficile mais comme vous avez dit : il faut tenter quand même on sait jamais .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy9l36sw3tpU"
      },
      "source": [
        "C'est quand même encore très bof.\r\n",
        "\r\n",
        "IL faudrait ré-entrainer avec des données plus solides d'A1 clairement. même si déjà c'est plus gérable (div par 10 du flag ?)\r\n",
        "\r\n",
        "Si on essaie avec le modèle Camembert ? \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1MaKld0pLw8"
      },
      "source": [
        "a1_test = tokenizer(np.array(df_a1_to_moderate[\"content\"]).tolist(), padding=True, truncation=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr6rLV3GqxjK"
      },
      "source": [
        "# print(a1_test[\"input_ids\"])\r\n",
        "# # predictions=\r\n",
        "a1_test_dataset=A1Dataset(a1_test,np.zeros(df_a1_to_moderate.shape[0]))\r\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/article_1_data/camembert.pt\"))\r\n",
        "predictions=[]\r\n",
        "\r\n",
        "test_loader=DataLoader(a1_test_dataset, batch_size=16, shuffle=False)\r\n",
        "with torch.no_grad():\r\n",
        "  for batch in test_loader:\r\n",
        "\r\n",
        "    inputs_test=batch[\"input_ids\"].to(device)\r\n",
        "    attention_mask_test=batch[\"attention_mask\"].to(device)\r\n",
        "    # print(model(inputs_test,attention_mask=attention_mask_test))\r\n",
        "    outputs=model(inputs_test,attention_mask=attention_mask_test)\r\n",
        "    predictions.append(np.argmax(outputs[0].cpu().data.numpy(), axis=1).tolist())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpRFc_kqAZLz",
        "outputId": "2df13920-482c-42c5-9bc2-41175b3c1a55"
      },
      "source": [
        "predictions=[item for sublist in predictions for item in sublist]\r\n",
        "print(predictions)\r\n",
        "\r\n",
        "print(sum(predictions))\r\n",
        "print(len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "51\n",
            "8233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLRyQjwQcuM9"
      },
      "source": [
        "IT TAKES AGES BUT GOD IT IS WORTH IT :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oDzWxQZctyw",
        "outputId": "6ae3a94a-de03-4c5c-8099-b317045ba0b0"
      },
      "source": [
        "indexes_camembert=[ i for i in range(len(predictions)) if predictions[i]==1]\n",
        "df_a1_bow_moderate=df_a1_to_moderate.iloc[indexes_camembert]\n",
        "print(indexes_camembert)\n",
        "\n",
        "for i in range(51): \n",
        "  print(df_a1_bow_moderate.iloc[i][\"content\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 77, 210, 316, 521, 534, 1124, 1273, 1538, 1849, 2008, 2169, 2267, 2286, 2287, 2291, 2313, 2347, 2498, 2499, 2763, 2911, 3194, 3497, 4001, 4199, 4200, 4201, 4204, 4206, 4208, 4211, 4213, 4214, 4217, 4220, 4227, 4701, 4783, 5459, 5628, 6095, 6112, 6149, 6504, 6507, 6512, 7092, 7199, 7805, 7853]\n",
            "t'es un amour merciiiiiiiiii\n",
            "*inébranlable est un caprice de mon correcteur automatique\n",
            "Hello Léane* !! correcteur farceur :)\n",
            "woaw t’es grave deter mdrr\n",
            "https://drive.google.com/file/d/1lkHW998wL4xGX-ptekBuJ09iroz_PMdz/view?usp=drivesdk\n",
            "Merci, tes réponses sont très complètes ! TooGoodToGo va devenir mon meilleur ami ahah\n",
            "La communication aha\n",
            "Après paris j’y penses paaas vrmnt pck j’veux pas partir non plus loin de chez moi, je suis vers cannes 06\n",
            "Quel hasard que tu soit à pissarro !\n",
            "D’accord merveaucoup pour l’aide\n",
            "Insta : nonoo__o \n",
            ":)\n",
            "Derien, il y a pas de quoi\n",
            "Mais oui sinon c’est abusé je suis d’accord mdrr\n",
            "Ok vas-y\n",
            "et oui vas-y passes sur insta\n",
            "Ah ouais ! En fait c’est compliqué ct’histoire mdrr\n",
            "mon insta c’est myrfrh_\n",
            "*Mélodie* oups sorry\n",
            "Ouais t’as raison ajoute moi sur snap youyoune93 je réponds à tes messages la bas c’est mieux\n",
            "Ouais c’est vrai que les notes veulent à la fois tout et rien dire mdrr\n",
            "T’a*\n",
            "Mais trop drôle ahahah\n",
            "ah bah niquel\n",
            "mais on vit le fait que ce soit bétonner de partout ? XD\n",
            "je tecoute!\n",
            "Et j'affiche pas mon corps sur mes reseaux et encore moins a quelqu un que je connais pas physiquement\n",
            "Grave 😂 elle va pas se laisser marcher dessus\n",
            "Ahh elle a du caractere la petite\n",
            "Pdt au moins 10 mess je t ai dit nonn nonn ....\n",
            "Mais la frnachement t as abuse\n",
            "En plus je suis sure t un bon gars\n",
            "Je sais meme pas pq t'insiste\n",
            "Y a un moment quand on dit non c non\n",
            "Okkay ouais t aller trop loin\n",
            "Bah ouais fallair pas forcer\n",
            "Snap vitoo-59 (juge pas aha) 😁\n",
            "Non mais attends déjà que j’ai l’impression d’être vieux à côtés des nouveaux a arrivants  alors si même ma génération me vouvoie 😂😭\n",
            "Ahh chacun son boulot mais le mien me plaît bien 😊\n",
            "\n",
            "C’est ce qu’il faut, éviter les blessures bêtes pcq y’a que ça qui peut bloquer \n",
            "Ouah une patriote surmotivée 😍😂\n",
            "voici mon snap : wawa.dia\n",
            "https://drive.google.com/file/d/1lkHW998wL4xGX-ptekBuJ09iroz_PMdz/view?usp=drivesdk\n",
            "SVT** et non pas svr dans le 2e message pardon\n",
            "Et mon insta c’est axel.dnr\n",
            "* époque moderne pour la renaissance\n",
            "Quand même, sinon personne ne tiendrait le coup 😂\n",
            "Deriiiien\n",
            "ajoute sur snap yasmp si j'ai un problème je te signale Mehdi pas de bêtise !\n",
            "S'il te plaît j'y suis jamais je veux bien ton snap\n",
            "Voici mon Facebook SZarah kr\n",
            "Oh okay\n",
            "Mon insta c’est sn.mariam\n",
            "Escuse moi\n",
            "Son instagram c’est @maellfe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApeeK81Wucb"
      },
      "source": [
        "## Creation of extra-features / Test a RF ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DjCdtW-WwRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}