{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project_A1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit ('base': conda)",
      "language": "python",
      "name": "python38264bitbaseconda67962706fddd4eb99e2936a1eb425e21"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.8.2-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmauryFaure/project_article_1/blob/master/Project_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SocQoXOEv9xd"
      },
      "source": [
        "# Projet Article 1 :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnG6hNo0PTCh",
        "outputId": "8c35ce32-358d-42b3-9173-9f17e0f4d086"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqifF1lXv9xt"
      },
      "source": [
        "#Importing pandas and numpy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8_xSR5-8I06",
        "outputId": "7e2aeedd-8e82-486f-8912-076812fe1d28"
      },
      "source": [
        "#Check python version\n",
        "!python --version"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps97S30yv9xu"
      },
      "source": [
        "## Creating the training Dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_hTvFZmTv9xu"
      },
      "source": [
        "#Get tweets\n",
        "df_to_moderate=pd.read_csv(\"/content/drive/MyDrive/article_1_data/selected_tweets.csv\")\n",
        "#Drop unused data\n",
        "df_to_moderate=df_to_moderate.drop([\"Id\",\"source\",\"Looked up word\"],axis=1)\n",
        "#Change column name\n",
        "df_to_moderate[\"content\"]=df_to_moderate[\"tweet (without @)\"]\n",
        "df_to_moderate=df_to_moderate.drop([\"tweet (without @)\"], axis=1)\n",
        "#Drop empty lines\n",
        "df_to_moderate=df_to_moderate.dropna()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oU7Wsxv9xw"
      },
      "source": [
        "#Get regular data\n",
        "df_article1_messages=pd.read_excel(\"/content/drive/MyDrive/article_1_data/AmauryModerationAllMessagesInspireFrom3Aout2020.xlsx\", sheet_name=\"ContenuNormal\")\n",
        "#Drop unused data\n",
        "df_article1_messages=df_article1_messages.drop([\"_id\",\"sender\",\"recipients.0\",\"threadId\",\"timestamp\",\"EchantillonNormal\"], axis=1)\n",
        "#Assign label\n",
        "df_article1_messages[\"Harmful\"]=0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZbR7w16v9xy"
      },
      "source": [
        "#Create training dataset. Balanced 50% for each class !\n",
        "df_to_moderate=pd.concat([df_article1_messages[:200],df_to_moderate],ignore_index=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53OrpKtv9xz"
      },
      "source": [
        "## Creating the test Datasets\n",
        "\n",
        "Problem : we do not have a balanced Dataset of real examples from article 1 for which we know the real distribution.\n",
        "\n",
        "What will we done instead :\n",
        "\n",
        "- Test on the dataset of MLMA tweets. \n",
        "- See % that come back on Article 1 dataset. \n",
        "\n",
        "### Test MLMA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ZCUPLVv9xz"
      },
      "source": [
        "test=pd.read_csv(\"/content/drive/MyDrive/article_1_data/fr_dataset_test.csv\")\n",
        "#Dropping Unused columns\n",
        "test=test.drop(test.columns[[7,8,9]],axis=1)\n",
        "test=test.drop(columns=[\"HITId\",\"directness\",\"annotator_sentiment\",\"target\",\"group\"])\n",
        "#Changing sentiment to 0 (for normal) and 1 (for else) \n",
        "test[\"sentiment\"]=[0 if test[\"sentiment\"][x]==\"normal\" else 1 for x in range(test.shape[0])]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAf3MrFYMbS9"
      },
      "source": [
        "### Test A1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MAY7ELRiMaib",
        "outputId": "bd3cc8be-5f3f-4495-b96f-63660fc7c3da"
      },
      "source": [
        "df_a1_to_moderate=pd.read_excel(\"/content/drive/MyDrive/article_1_data/AmauryModerationAllMessagesInspireFrom3Aout2020.xlsx\", sheet_name=\"ContenuToModerate\")\n",
        "df_a1_to_moderate=df_a1_to_moderate.drop(columns=[\"_id\",\"sender\",\"recipients.0\",\"threadId\",\"timestamp\",\"EchantillonToModerate\"])\n",
        "# df_a1_to_moderate.columns=df_a1_to_moderate.columns.str.replace(\"\\n\",\" \")\n",
        "df_a1_to_moderate.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non, la physique-chimie de lycée n’a pas grand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Merci infiniment d'avoir pris autant de temps ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bonjour ! Alors non la SVT et la biologie ce n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>salut\\nJ'aimerais savoir ce qu'est exactement ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>je sais pas si j'ai répondu à toutes tes quest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content\n",
              "0  Non, la physique-chimie de lycée n’a pas grand...\n",
              "1  Merci infiniment d'avoir pris autant de temps ...\n",
              "2  Bonjour ! Alors non la SVT et la biologie ce n...\n",
              "3  salut\\nJ'aimerais savoir ce qu'est exactement ...\n",
              "4  je sais pas si j'ai répondu à toutes tes quest..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsIDQYbzNGjQ"
      },
      "source": [
        "## Creating Fine-tuning Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jyd-6kubNKaB",
        "outputId": "65e8f5fa-f030-4e82-b220-91b4a2c03e65"
      },
      "source": [
        "# df_a1_fine_tune=pd.DataFrame(pd.concat([df_a1_to_moderate[\"content\"],df_article1_messages[\"content\"]]))\n",
        "df_a1_fine_tune=pd.read_excel(\"/content/drive/MyDrive/article_1_data/AmauryModerationAllMessagesInspireFrom3Aout2020.xlsx\", sheet_name=\"AllMessagesInspireFrom3Aout2020\")\n",
        "df_a1_fine_tune=df_a1_fine_tune.drop(columns=[\"_id\",\"sender\",\"recipients.0\",\"threadId\",\"timestamp\",\"EchantillonToModerate\",\"EchantillonNormal\"])\n",
        "df_a1_fine_tune.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bonsoir, j’ai une formation dans l’aménagement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tu peux me parler un peu de l’apprentissage de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bonjour, \\nTu veux te tourner vers quel parcou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bonjour \\nLa première année après bac peut fai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bonsoir, je me présente je suis élève de Termi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content\n",
              "0  Bonsoir, j’ai une formation dans l’aménagement...\n",
              "1  Tu peux me parler un peu de l’apprentissage de...\n",
              "2  Bonjour, \\nTu veux te tourner vers quel parcou...\n",
              "3  Bonjour \\nLa première année après bac peut fai...\n",
              "4  Bonsoir, je me présente je suis élève de Termi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQaZ3Dhwv9x0"
      },
      "source": [
        "## Implementing BoW and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdCToMlLv9x0"
      },
      "source": [
        "#Creating arrays for test_values and labels\n",
        "test_values=np.array(test[\"tweet\"])\n",
        "test_labels=np.array(test[\"sentiment\"])\n",
        "# Same but for training\n",
        "train_values=np.array(df_to_moderate[\"content\"])\n",
        "#Extra step to Making sure everything is on a string format (some numbers might not be)\n",
        "train_values=[str(train_values[i]) for i in range(len(train_values))]\n",
        "train_labels=np.array(df_to_moderate[\"Harmful\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8lMYH_v9x0"
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "#Import SGDClassifier (allows to implement logistic regression)\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#Import matplotlib for plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#Import some metrics\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
        "#Import seaborn for plots\n",
        "import seaborn as sns\n",
        "\n",
        "#This function comes from : https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2\n",
        "#It computes F1-score\n",
        "def calc_f1(p_and_r):\n",
        "    p, r = p_and_r\n",
        "    return (2*p*r)/(p+r)\n",
        "\n",
        "#This function comes from : https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2\n",
        "#It computes some metrics\n",
        "def compute_metrics(y_test, y_prob, verbose=False, return_metrics=True, confusion=False):\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_prob, pos_label = 1)\n",
        "\n",
        "  #Optimizing the F1-score\n",
        "  best_f1_index =np.argmax([calc_f1(p_r) for p_r in zip(precision, recall)])\n",
        "  best_threshold, best_precision, best_recall = threshold[best_f1_index], precision[best_f1_index], recall[best_f1_index]\n",
        "\n",
        "  y_test_pred = np.where(y_prob > best_threshold, 1, 0)\n",
        "  \n",
        "  f1 = f1_score(y_test, y_test_pred, pos_label = 1, average = 'binary')\n",
        "  roc_auc = roc_auc_score(y_test, y_prob)\n",
        "  acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "  if confusion:\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(cm)\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    sns.set(font_scale=1.0) #for label size\n",
        "    sns.heatmap(cm, annot = True, fmt = 'd', xticklabels = ['Not harmful', 'Harmful'], yticklabels = ['Not harmful', 'Harmful'], annot_kws={\"size\": 14}, cmap = 'Blues')# font size\n",
        "\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Truth')\n",
        "\n",
        "  if verbose:\n",
        "    print('F1: {:.3f} | Pr: {:.3f} | Re: {:.3f} | AUC: {:.3f} | Accuracy: {:.3f} \\n'.format(f1, best_precision, best_recall, roc_auc, acc))\n",
        "    \n",
        "  if return_metrics:\n",
        "    return np.array([f1, best_precision, best_recall, roc_auc, acc])\n",
        "\n",
        "#Defining a function to do 10 times the logistic regression\n",
        "def evaluate_log_reg(train_features, test_features, y_train, y_test):\n",
        "    score=0\n",
        "    metrics = np.zeros(5)\n",
        "    for i in range(10):\n",
        "        log_reg=SGDClassifier(loss=\"log\", penalty='l2')\n",
        "        log_reg.fit(train_features,y_train)\n",
        "      \n",
        "        y_prob=log_reg.predict_proba(test_features)[:,1]\n",
        "        metrics+=compute_metrics(y_test,y_prob)\n",
        "    metrics /=10\n",
        "\n",
        "    return metrics\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_J22Bvv9x0",
        "outputId": "699fd784-a5c0-4548-c4ca-0e514b2ea7d6"
      },
      "source": [
        "# Bag of Word\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Declaring the vectorizer\n",
        "bow=CountVectorizer()\n",
        "#Fiting the model to the training dataset, and vectorizing the training dataset\n",
        "train_bow=bow.fit_transform(train_values)\n",
        "#Vectorizing test dataset\n",
        "test_bow=bow.transform(test_values)\n",
        "#Real metrics\n",
        "metrics_bow=evaluate_log_reg(train_bow, test_bow, train_labels, test_labels)\n",
        "#Metrics evaluated on train dataset (to see if overfitting)\n",
        "metrics_train=evaluate_log_reg(train_bow, train_bow, train_labels, train_labels)\n",
        "print(metrics_bow)\n",
        "print(metrics_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.88635199 0.79692655 0.9987948  0.59982261 0.79634208]\n",
            "[0.99749373 1.         1.         1.         0.9975    ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxjIM_xcv9x1",
        "outputId": "a47dc17b-4019-4e05-f3c5-064b811785ba"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf=TfidfVectorizer()\n",
        "train_tfidf=tfidf.fit_transform(train_values)\n",
        "test_tfidf=tfidf.transform(test_values)\n",
        "\n",
        "metrics_tfidf=evaluate_log_reg(train_tfidf, test_tfidf, train_labels, test_labels)\n",
        "metrics_train_tfidf=evaluate_log_reg(train_tfidf, train_tfidf, train_labels, train_labels)\n",
        "print(metrics_tfidf)\n",
        "print(metrics_train_tfidf)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.88656571 0.79764834 0.99816048 0.65224214 0.79689707]\n",
            "[0.99749373 1.         1.         1.         0.9975    ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvX7M3I4v9x4"
      },
      "source": [
        "### Implementing camembert : First Attempt :\n",
        "\n",
        "In this attempt we are going to use transformers and PyTorch, using the latter for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMt56ub2ycLp",
        "outputId": "6525a67e-b0fb-4494-939c-aa22190b53e0"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZJ4qQFv9x4"
      },
      "source": [
        "#Importing function to split training data into validation and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_values, train_labels, test_size=.2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxo04C5ov9x4"
      },
      "source": [
        "#Tokeinization for camembert\n",
        "from transformers import CamembertTokenizer\n",
        "tokenizer=CamembertTokenizer.from_pretrained(\"camembert-base\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iRHGAFv9x4"
      },
      "source": [
        "#Tokenizing\n",
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True,truncation=True)\n",
        "test_encodings = tokenizer(test_values.tolist(), padding=True, truncation=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1DSz2W_v9x5"
      },
      "source": [
        "#Importing pyTorch\n",
        "import torch\n",
        "\n",
        "class A1Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.encodings.items())\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odufWocEv9x6",
        "outputId": "46a6c1c7-48ce-4e9f-9553-91fc33b480a7"
      },
      "source": [
        "#Utility to load dataset in batch\n",
        "from torch.utils.data import DataLoader\n",
        "#Importing camembdert and AdamW Optimizer\n",
        "from transformers import CamembertForSequenceClassification, AdamW\n",
        "\n",
        "#Model declaration\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "#Training\n",
        "for epoch in range(1):\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.long())\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Ti767eguQZry",
        "outputId": "aeba4491-fddd-475b-c8e7-07d42a79f3c7"
      },
      "source": [
        "#Evaluating the model : \n",
        "with torch.no_grad():\n",
        "  input_ids = test_dataset[:]['input_ids'].to(device)\n",
        "  attention_mask = test_dataset[:]['attention_mask'].to(device)\n",
        "  labels = test_dataset[:]['labels'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  metrics_camembert=compute_metrics(labels.cpu().data.numpy(),outputs[0].cpu().data.numpy()[:,1], confusion=True)\n",
        "  print(metrics_camembert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  32  779]\n",
            " [  21 3132]]\n",
            "[0.88674972 0.80086912 0.99365683 0.68930516 0.79818365]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Sa00rwsDv7"
      },
      "source": [
        "metrics_df=pd.DataFrame(metrics_bow.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "df2=pd.DataFrame(metrics_tfidf.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "df3=pd.DataFrame(metrics_camembert.reshape((1,-1)), columns=[\"f1\", \"best_precision\", \"best_recall\", \"roc_auc\", \"acc\"])\n",
        "metrics_df=metrics_df.append(df2,ignore_index=True)\n",
        "metrics_df=metrics_df.append(df3,ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "JxwhcYazbtI8",
        "outputId": "97801f1f-864c-4724-8fd1-7346f8cfeb2f"
      },
      "source": [
        "metrics_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>best_precision</th>\n",
              "      <th>best_recall</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.886337</td>\n",
              "      <td>0.796855</td>\n",
              "      <td>0.998858</td>\n",
              "      <td>0.598020</td>\n",
              "      <td>0.796292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.886563</td>\n",
              "      <td>0.797442</td>\n",
              "      <td>0.998509</td>\n",
              "      <td>0.651417</td>\n",
              "      <td>0.796821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.884426</td>\n",
              "      <td>0.795409</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.548913</td>\n",
              "      <td>0.798184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1  best_precision  best_recall   roc_auc       acc\n",
              "0  0.886337        0.796855     0.998858  0.598020  0.796292\n",
              "1  0.886563        0.797442     0.998509  0.651417  0.796821\n",
              "2  0.884426        0.795409     1.000000  0.548913  0.798184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyQJg1i-OkzN"
      },
      "source": [
        "On n'a pas de meilleure performance selon le modèle choisi. \n",
        "Etonnament le modèle camembert donne une aire sous la courbe ROC plus faible que les autres, alors que le reste des indicateurs sont similaires.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDp2Po0MeMja"
      },
      "source": [
        "## Implementing Camembert : Second version :\n",
        "\n",
        "Cette fois-ci on utilise toujours Transformers et PyTorch mais on utilise des fonctions plus haut niveau de Transformers. \n",
        "\n",
        "On va aussi tenter de fonctionner en deux étapes :\n",
        "- \"Language Modeling Fine-Tuning\" : Adapter la représentation de la langue de CamemBERT au 47k messages d'Article 1\n",
        "-Classifier (même méthode).\n",
        "\n",
        "### Partie Language Modeling Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irEoBGTLXxbU"
      },
      "source": [
        "# print(df_a1_fine_tune[\"content\"].values.tolist())\n",
        "fine_tune_values=df_a1_fine_tune[\"content\"].values\n",
        "fine_tune_values=[str(fine_tune_values[i]) for i in range(len(fine_tune_values))]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "RQtJizZdMEtV",
        "outputId": "07c099bc-234c-4a20-f7d0-235660327a0d"
      },
      "source": [
        "#Creating dataset for fine-tuning\n",
        "fine_tuning_encodings=tokenizer(fine_tune_values,padding=True, truncation=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-39a6827b56f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Creating dataset for fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfine_tuning_encodings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_tune_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfine_tuning_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_tuning_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmFI5xfPZu7z"
      },
      "source": [
        "fine_tuning_dataset=A1Dataset(fine_tuning_encodings,np.zeros((len(fine_tune_values))))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXD9M3mJLRfF"
      },
      "source": [
        "from transformers import CamembertModel, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "fine_tune_training_args = TrainingArguments(\n",
        "    output_dir='./results/fine',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs/fine',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "fine_tune_model=CamembertModel.from_pretrained(\"camembert-base\")\n",
        "\n",
        "fine_tune_trainer=Trainer(\n",
        "    model=fine_tune_model,\n",
        "    args=fine_tune_training_args,\n",
        "    train_dataset=fine_tuning_dataset\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "JV872CnMZ_RZ",
        "outputId": "1ca39dd1-ba86-4679-d233-ce09f4ec4b00"
      },
      "source": [
        "fine_tune_trainer.train()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-85a68f17bd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5_EGCiwiNiM"
      },
      "source": [
        "### Partie classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ_Z3uJFMDVh"
      },
      "source": [
        "#Crating dataset for Classification\n",
        "train_dataset =  A1Dataset(train_encodings, train_labels)\n",
        "val_dataset =  A1Dataset(val_encodings, val_labels)\n",
        "test_dataset =  A1Dataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "1R00kW3qv9x5",
        "outputId": "8c10187a-6d2c-4ab3-bc75-e51b5ad6f994"
      },
      "source": [
        "from transformers import CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.687700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.663100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=0.6803353706995646, metrics={'train_runtime': 42.0907, 'train_samples_per_second': 1.425, 'total_flos': 170130077740800, 'epoch': 3.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "6EjBmTUqFNT1",
        "outputId": "84c28e31-7b91-4ee1-86ac-cbc939f12199"
      },
      "source": [
        "predictions =trainer.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='124' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [62/62 01:47]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6iIjxy9FkLg",
        "outputId": "0fbb6350-842c-4d1d-c3ef-0864d014a884"
      },
      "source": [
        "print(predictions[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gV5hXn6F6JB",
        "outputId": "171ac416-369a-41c5-f5cb-69f83dea916d"
      },
      "source": [
        "accuracy=accuracy_score(test_dataset.labels,predictions[1])\n",
        "f1=f1_score(test_dataset.labels,predictions[1])\n",
        "print(accuracy)\n",
        "print(f1)\n",
        "print(test_dataset.labels)\n",
        "print(predictions[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "[1 1 1 ... 1 0 1]\n",
            "[1 1 1 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANDnIGL2yMc9"
      },
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/article_1_model/camembert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoaQK2ZriT3j"
      },
      "source": [
        "Ce modèle semble meilleur sur le dataset MLMA (précision à 1, F1-score à 1, classifieur parfait). Possibilité d'overfit ? (cela semble confirmé par le 0 message qui ressort en fin de notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tDCa0DmxiSt"
      },
      "source": [
        "## Analyse : Quelles implications sur les messages à modérer A1 ? \r\n",
        "\r\n",
        "### Test avec BoW+ Log Reg\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO_9WPqD0Fjm"
      },
      "source": [
        "test_values=np.array(test[\"tweet\"])\r\n",
        "test_labels=np.array(test[\"sentiment\"])\r\n",
        "\r\n",
        "train_values=np.array(df_to_moderate[\"content\"])\r\n",
        "train_values=[str(train_values[i]) for i in range(len(train_values))]\r\n",
        "train_labels=np.array(df_to_moderate[\"Harmful\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfgEdBDFxpO4"
      },
      "source": [
        "\r\n",
        "bow=CountVectorizer()\r\n",
        "train_bow=bow.fit_transform(train_values)\r\n",
        "test_bow=bow.transform(test_values)\r\n",
        "\r\n",
        "# metrics_bow=evaluate_log_reg(train_bow, test_bow, train_labels, test_labels)\r\n",
        "\r\n",
        "log_reg=SGDClassifier(loss=\"log\", penalty='l2')\r\n",
        "log_reg.fit(train_bow,train_labels)\r\n",
        "\r\n",
        "prediction=log_reg.predict(bow.transform(df_a1_to_moderate[\"content\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P1DrhvJ0XQk",
        "outputId": "88f0439f-1d49-498b-c53d-56de46c1ae6d"
      },
      "source": [
        "print(prediction)\r\n",
        "print(f\"Le modèle fait ressortir {sum(prediction)} messages problématiques\")\r\n",
        "print(f\"Nombre de messages problématiques originaux : {df_a1_to_moderate.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "Le modèle fait ressortir 944.0 messages problématiques\n",
            "Nombre de messages problématiques originaux : 8233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Rs8zgz1L1N",
        "outputId": "fc6f06ce-807c-443e-e53c-c987036a8880"
      },
      "source": [
        "indexes=[ i for i in range(len(prediction)) if prediction[i]==1]\r\n",
        "df_a1_bow_moderate=df_a1_to_moderate.iloc[indexes]\r\n",
        "print(indexes)\r\n",
        "\r\n",
        "for i in range(10): \r\n",
        "  print(df_a1_bow_moderate.iloc[i][\"content\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 36, 45, 52, 77, 80, 82, 88, 90, 141, 143, 145, 164, 165, 172, 190, 194, 204, 210, 220, 233, 236, 243, 247, 252, 255, 266, 275, 281, 282, 298, 299, 308, 316, 327, 331, 333, 335, 337, 338, 340, 343, 353, 365, 375, 387, 388, 431, 437, 442, 450, 453, 464, 471, 480, 491, 493, 499, 504, 505, 521, 526, 527, 528, 550, 558, 562, 566, 570, 571, 572, 577, 591, 595, 641, 642, 646, 649, 654, 656, 693, 694, 697, 698, 699, 700, 701, 729, 749, 757, 763, 770, 776, 779, 799, 803, 809, 829, 837, 838, 865, 904, 912, 913, 916, 918, 957, 958, 963, 976, 983, 987, 1040, 1041, 1043, 1046, 1054, 1056, 1071, 1081, 1083, 1094, 1097, 1108, 1109, 1116, 1124, 1126, 1129, 1130, 1138, 1139, 1144, 1151, 1170, 1186, 1192, 1220, 1247, 1262, 1270, 1271, 1281, 1291, 1300, 1305, 1308, 1313, 1319, 1329, 1377, 1386, 1388, 1393, 1410, 1421, 1426, 1430, 1447, 1452, 1458, 1460, 1472, 1480, 1482, 1493, 1534, 1541, 1542, 1550, 1561, 1564, 1569, 1579, 1602, 1607, 1609, 1612, 1618, 1622, 1636, 1688, 1696, 1698, 1699, 1702, 1705, 1706, 1711, 1721, 1725, 1737, 1740, 1741, 1749, 1765, 1767, 1830, 1852, 1856, 1866, 1869, 1877, 1891, 1897, 1912, 1915, 1916, 1917, 1918, 1919, 1931, 1934, 1940, 1954, 1955, 1959, 1965, 1983, 1990, 1993, 2007, 2008, 2009, 2021, 2029, 2064, 2068, 2076, 2080, 2083, 2090, 2093, 2095, 2103, 2104, 2106, 2107, 2116, 2131, 2138, 2140, 2143, 2169, 2175, 2184, 2198, 2199, 2214, 2217, 2219, 2220, 2229, 2232, 2248, 2266, 2268, 2271, 2288, 2289, 2293, 2295, 2298, 2299, 2302, 2306, 2307, 2309, 2311, 2313, 2320, 2344, 2347, 2359, 2362, 2383, 2385, 2408, 2409, 2412, 2420, 2432, 2445, 2446, 2455, 2461, 2465, 2487, 2491, 2497, 2498, 2500, 2501, 2503, 2504, 2508, 2513, 2522, 2527, 2528, 2538, 2539, 2541, 2544, 2545, 2554, 2560, 2566, 2585, 2592, 2612, 2653, 2655, 2691, 2693, 2699, 2703, 2707, 2719, 2720, 2721, 2723, 2750, 2763, 2791, 2794, 2812, 2817, 2822, 2823, 2832, 2835, 2836, 2872, 2877, 2880, 2881, 2886, 2887, 2891, 2892, 2893, 2911, 2912, 2923, 2925, 2965, 2975, 2979, 3006, 3012, 3027, 3049, 3065, 3078, 3087, 3088, 3089, 3092, 3097, 3099, 3100, 3111, 3128, 3136, 3139, 3141, 3152, 3167, 3193, 3194, 3196, 3211, 3225, 3226, 3227, 3243, 3246, 3247, 3248, 3250, 3254, 3262, 3271, 3272, 3299, 3300, 3301, 3302, 3310, 3334, 3338, 3344, 3372, 3386, 3393, 3395, 3426, 3427, 3429, 3444, 3474, 3484, 3494, 3497, 3499, 3508, 3514, 3533, 3534, 3536, 3543, 3544, 3558, 3560, 3563, 3573, 3581, 3587, 3600, 3607, 3620, 3624, 3646, 3648, 3650, 3664, 3684, 3693, 3699, 3704, 3706, 3710, 3718, 3729, 3736, 3738, 3747, 3749, 3750, 3773, 3780, 3790, 3795, 3807, 3831, 3845, 3853, 3856, 3857, 3867, 3869, 3874, 3880, 3904, 3908, 3914, 3918, 3946, 3953, 3984, 3986, 3987, 3988, 3989, 3993, 4001, 4005, 4006, 4007, 4008, 4015, 4029, 4041, 4045, 4050, 4057, 4058, 4091, 4097, 4107, 4113, 4119, 4127, 4129, 4136, 4146, 4149, 4150, 4151, 4154, 4155, 4158, 4160, 4162, 4169, 4170, 4173, 4175, 4184, 4190, 4198, 4200, 4201, 4204, 4206, 4207, 4209, 4210, 4211, 4212, 4213, 4217, 4218, 4219, 4220, 4221, 4222, 4226, 4227, 4240, 4258, 4278, 4288, 4292, 4294, 4303, 4321, 4325, 4327, 4338, 4355, 4378, 4392, 4396, 4398, 4401, 4413, 4422, 4423, 4432, 4465, 4476, 4480, 4481, 4496, 4498, 4503, 4504, 4508, 4514, 4531, 4532, 4561, 4562, 4566, 4570, 4571, 4573, 4574, 4584, 4605, 4619, 4636, 4653, 4654, 4675, 4683, 4686, 4690, 4696, 4701, 4702, 4708, 4776, 4780, 4782, 4783, 4785, 4803, 4804, 4806, 4811, 4812, 4816, 4825, 4826, 4882, 4885, 4887, 4891, 4918, 4926, 4927, 4928, 4936, 4943, 4947, 4964, 4967, 4968, 4974, 4992, 5005, 5018, 5027, 5028, 5030, 5039, 5072, 5078, 5083, 5094, 5095, 5116, 5119, 5121, 5122, 5123, 5126, 5127, 5134, 5143, 5149, 5158, 5169, 5176, 5180, 5187, 5189, 5194, 5196, 5202, 5205, 5207, 5228, 5249, 5270, 5283, 5328, 5337, 5342, 5347, 5351, 5352, 5356, 5357, 5358, 5363, 5364, 5374, 5397, 5398, 5411, 5431, 5436, 5441, 5443, 5447, 5459, 5464, 5465, 5470, 5483, 5493, 5519, 5526, 5527, 5530, 5541, 5544, 5580, 5601, 5603, 5606, 5613, 5627, 5637, 5639, 5640, 5643, 5661, 5664, 5688, 5691, 5696, 5698, 5699, 5700, 5703, 5763, 5771, 5774, 5784, 5787, 5792, 5793, 5796, 5801, 5836, 5850, 5854, 5855, 5856, 5859, 5870, 5877, 5879, 5882, 5887, 5888, 5890, 5903, 5909, 5922, 5925, 5969, 5985, 6038, 6041, 6055, 6063, 6074, 6078, 6086, 6088, 6095, 6104, 6107, 6112, 6113, 6117, 6118, 6124, 6127, 6128, 6131, 6132, 6133, 6149, 6153, 6158, 6162, 6177, 6183, 6186, 6189, 6205, 6206, 6210, 6249, 6271, 6289, 6313, 6320, 6334, 6354, 6376, 6381, 6390, 6394, 6420, 6423, 6429, 6436, 6444, 6451, 6453, 6457, 6484, 6488, 6489, 6491, 6499, 6505, 6507, 6512, 6519, 6528, 6530, 6534, 6555, 6572, 6603, 6611, 6621, 6641, 6652, 6671, 6672, 6675, 6686, 6688, 6690, 6693, 6697, 6707, 6709, 6723, 6754, 6755, 6776, 6787, 6810, 6815, 6845, 6873, 6877, 6883, 6889, 6904, 6917, 6922, 6924, 6925, 6926, 6930, 6931, 6936, 6938, 6940, 6944, 6957, 6995, 7003, 7029, 7039, 7049, 7052, 7075, 7091, 7092, 7096, 7097, 7099, 7100, 7107, 7113, 7145, 7168, 7198, 7199, 7201, 7205, 7207, 7212, 7232, 7267, 7272, 7283, 7295, 7298, 7315, 7342, 7365, 7369, 7373, 7374, 7377, 7382, 7405, 7406, 7416, 7421, 7478, 7480, 7482, 7484, 7489, 7492, 7501, 7509, 7547, 7553, 7555, 7569, 7602, 7603, 7604, 7623, 7624, 7642, 7643, 7667, 7670, 7680, 7694, 7704, 7715, 7757, 7775, 7779, 7787, 7804, 7805, 7806, 7807, 7815, 7831, 7834, 7838, 7841, 7845, 7847, 7848, 7849, 7850, 7853, 7854, 7855, 7873, 7888, 7893, 7898, 7907, 7909, 7915, 7972, 7982, 7983, 7985, 7986, 7987, 7993, 7996, 8012, 8027, 8033, 8034, 8042, 8068, 8070, 8108, 8115, 8129, 8143, 8147, 8148, 8162, 8170, 8205, 8213, 8217, 8219]\n",
            "Non j'étais logée chez mes parents \n",
            "Mais sinon je vais qu'il y a pas mal de résidences étudiantes autour de l'IUT\n",
            "t'es un amour merciiiiiiiiii\n",
            "Le double diplôme archi ingé ça m'intéresse mais ça a l'air d'être hyper sélectif...\n",
            "Mais ducoup ça porte sur quoi les oraux ?\n",
            "A Lyon Est le tutorat est génial et remplace complètement une prepa! Ils ont des polycopiés complets, des cours du soir qui aident vraiment et des épreuves blanches chaque semaine.\n",
            "Je n'avais pas les moyens non plus de prendre une prépa, j'ai fait mon année qu'avec le tutorat et j'ai réussi à valider médecine et sage-femme donc c'est largement faisable sans prepa!\n",
            "\n",
            "Je vais finir par croire qu'on est les deux mêmes 😂\n",
            "\n",
            "C'est exactement ça c'est une année intensive où tu te consacres uniquement à ca mais ça vaut le coup! Et ce n'est qu'un an!\n",
            "Et comme tu dis tu auras toujours une deuxième chance\n",
            "*inébranlable est un caprice de mon correcteur automatique\n",
            "Je vois ! Je fais à la fois inébranlable licence d’histoire et une licence LLCER espagnol\n",
            "Bonjour, par le même domaine tu veux dire la double licence histoire-llcer espagnol ?\n",
            "Mais est-ce que les transports en communs sont bien genre je peux aller rapidement d'un endroit à un autre si ils ne sont pas trop encombrés???\n",
            "D'accord merci c'est vrai que ducoup avec une amie on voudrait peut-être faire une coloc donc voilà. Mais sinon une journée type à quoi est-ce que ça ressemble avec le rythme et l'ambiance\n",
            "La langue espagnol a une autre origine comme par exemple le français au moyen age c'était le latin qui été a la place du français\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy9l36sw3tpU"
      },
      "source": [
        "C'est peut satisfaisant, on remarque trop messages qui sont totalement normaux et pour lesquels on ne peut pas trouver de raison de modération. \r\n",
        "\r\n",
        "Il faudrait ré-entrainer avec des données plus précises d'Article 1. L'approche ne semble a priori pas très prometteuse (même si son coût étant très limité peut valoir l'approche). \r\n",
        "\r\n",
        "### Test avec Camembert v1 :\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1MaKld0pLw8"
      },
      "source": [
        "a1_test = tokenizer(np.array(df_a1_to_moderate[\"content\"]).tolist(), padding=True, truncation=True)\r\n",
        "a1_test_dataset=A1Dataset(a1_test,np.zeros(df_a1_to_moderate.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr6rLV3GqxjK"
      },
      "source": [
        "# print(a1_test[\"input_ids\"])\r\n",
        "# # predictions=\r\n",
        "\r\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/article_1_data/camembert.pt\"))\r\n",
        "predictions=[]\r\n",
        "\r\n",
        "test_loader=DataLoader(a1_test_dataset, batch_size=16, shuffle=False)\r\n",
        "with torch.no_grad():\r\n",
        "  for batch in test_loader:\r\n",
        "\r\n",
        "    inputs_test=batch[\"input_ids\"].to(device)\r\n",
        "    attention_mask_test=batch[\"attention_mask\"].to(device)\r\n",
        "    # print(model(inputs_test,attention_mask=attention_mask_test))\r\n",
        "    outputs=model(inputs_test,attention_mask=attention_mask_test)\r\n",
        "    predictions.append(np.argmax(outputs[0].cpu().data.numpy(), axis=1).tolist())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpRFc_kqAZLz",
        "outputId": "2df13920-482c-42c5-9bc2-41175b3c1a55"
      },
      "source": [
        "predictions=[item for sublist in predictions for item in sublist]\r\n",
        "print(predictions)\r\n",
        "\r\n",
        "print(f\"Le modèle fait ressortir {sum(predictions)} messages problématiques\")\r\n",
        "print(f\"Sur {len(predictions)} messages\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "51\n",
            "8233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oDzWxQZctyw",
        "outputId": "6ae3a94a-de03-4c5c-8099-b317045ba0b0"
      },
      "source": [
        "indexes_camembert=[ i for i in range(len(predictions)) if predictions[i]==1]\n",
        "df_a1_bow_moderate=df_a1_to_moderate.iloc[indexes_camembert]\n",
        "print(indexes_camembert)\n",
        "\n",
        "for i in range(51): \n",
        "  print(df_a1_bow_moderate.iloc[i][\"content\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 77, 210, 316, 521, 534, 1124, 1273, 1538, 1849, 2008, 2169, 2267, 2286, 2287, 2291, 2313, 2347, 2498, 2499, 2763, 2911, 3194, 3497, 4001, 4199, 4200, 4201, 4204, 4206, 4208, 4211, 4213, 4214, 4217, 4220, 4227, 4701, 4783, 5459, 5628, 6095, 6112, 6149, 6504, 6507, 6512, 7092, 7199, 7805, 7853]\n",
            "t'es un amour merciiiiiiiiii\n",
            "*inébranlable est un caprice de mon correcteur automatique\n",
            "Hello Léane* !! correcteur farceur :)\n",
            "woaw t’es grave deter mdrr\n",
            "https://drive.google.com/file/d/1lkHW998wL4xGX-ptekBuJ09iroz_PMdz/view?usp=drivesdk\n",
            "Merci, tes réponses sont très complètes ! TooGoodToGo va devenir mon meilleur ami ahah\n",
            "La communication aha\n",
            "Après paris j’y penses paaas vrmnt pck j’veux pas partir non plus loin de chez moi, je suis vers cannes 06\n",
            "Quel hasard que tu soit à pissarro !\n",
            "D’accord merveaucoup pour l’aide\n",
            "Insta : nonoo__o \n",
            ":)\n",
            "Derien, il y a pas de quoi\n",
            "Mais oui sinon c’est abusé je suis d’accord mdrr\n",
            "Ok vas-y\n",
            "et oui vas-y passes sur insta\n",
            "Ah ouais ! En fait c’est compliqué ct’histoire mdrr\n",
            "mon insta c’est myrfrh_\n",
            "*Mélodie* oups sorry\n",
            "Ouais t’as raison ajoute moi sur snap youyoune93 je réponds à tes messages la bas c’est mieux\n",
            "Ouais c’est vrai que les notes veulent à la fois tout et rien dire mdrr\n",
            "T’a*\n",
            "Mais trop drôle ahahah\n",
            "ah bah niquel\n",
            "mais on vit le fait que ce soit bétonner de partout ? XD\n",
            "je tecoute!\n",
            "Et j'affiche pas mon corps sur mes reseaux et encore moins a quelqu un que je connais pas physiquement\n",
            "Grave 😂 elle va pas se laisser marcher dessus\n",
            "Ahh elle a du caractere la petite\n",
            "Pdt au moins 10 mess je t ai dit nonn nonn ....\n",
            "Mais la frnachement t as abuse\n",
            "En plus je suis sure t un bon gars\n",
            "Je sais meme pas pq t'insiste\n",
            "Y a un moment quand on dit non c non\n",
            "Okkay ouais t aller trop loin\n",
            "Bah ouais fallair pas forcer\n",
            "Snap vitoo-59 (juge pas aha) 😁\n",
            "Non mais attends déjà que j’ai l’impression d’être vieux à côtés des nouveaux a arrivants  alors si même ma génération me vouvoie 😂😭\n",
            "Ahh chacun son boulot mais le mien me plaît bien 😊\n",
            "\n",
            "C’est ce qu’il faut, éviter les blessures bêtes pcq y’a que ça qui peut bloquer \n",
            "Ouah une patriote surmotivée 😍😂\n",
            "voici mon snap : wawa.dia\n",
            "https://drive.google.com/file/d/1lkHW998wL4xGX-ptekBuJ09iroz_PMdz/view?usp=drivesdk\n",
            "SVT** et non pas svr dans le 2e message pardon\n",
            "Et mon insta c’est axel.dnr\n",
            "* époque moderne pour la renaissance\n",
            "Quand même, sinon personne ne tiendrait le coup 😂\n",
            "Deriiiien\n",
            "ajoute sur snap yasmp si j'ai un problème je te signale Mehdi pas de bêtise !\n",
            "S'il te plaît j'y suis jamais je veux bien ton snap\n",
            "Voici mon Facebook SZarah kr\n",
            "Oh okay\n",
            "Mon insta c’est sn.mariam\n",
            "Escuse moi\n",
            "Son instagram c’est @maellfe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzY9oF0AfwL1"
      },
      "source": [
        "Beaucoup moins de messages ressortent, et surtout les messages semblent plus cohérent (présence de snapchat, insta, facebook ou encore smiley). On note cependant que cela reste des messages relativement \"innocents\".\n",
        "\n",
        "### Test avec Camembert v2:\n",
        "\n",
        "Test préliminaire sans l'étape de language modeling adaptation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "NKpaWinKzeA7",
        "outputId": "e962c9b9-6bfc-46e5-dea2-c5c41952910c"
      },
      "source": [
        "predictions_a1=trainer.predict(a1_test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='253' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [62/62 14:50]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_AGRceCI3e2",
        "outputId": "3ea2de27-bab1-4049-8fdc-a9da6e6c53b3"
      },
      "source": [
        "print(sum(predictions_a1[1]))\n",
        "print(predictions_a1[1])  \n",
        "# print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqX0XdZheIe"
      },
      "source": [
        "On remarque ici que ce nouvel algorithme ne détecte aucun cas de messages à modérer. C'est potentiellement car les messages d'entrainement sont trop violent ? "
      ]
    }
  ]
}